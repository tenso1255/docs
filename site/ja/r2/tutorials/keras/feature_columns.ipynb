{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rNdWfPXCjTjY"
   },
   "source": [
    "##### Copyright 2019 The TensorFlow Authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "I1dUQ0GejU8N"
   },
   "outputs": [],
   "source": [
    "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c05P9g5WjizZ"
   },
   "source": [
    "# Classify structured data\n",
    "# 構造化されたデータの分類"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zofH_gCzgplN"
   },
   "source": [
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://www.tensorflow.org/alpha/tutorials/keras/feature_columns\">\n",
    "    <img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />\n",
    "    View on TensorFlow.org</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs/blob/master/site/ja/r2/tutorials/keras/feature_columns.ipynb\">\n",
    "    <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />\n",
    "    Run in Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/tensorflow/docs/blob/master/site/ja/r2/tutorials/keras/feature_columns.ipynb\">\n",
    "    <img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />\n",
    "    View source on GitHub</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K1y4OHpGgss7"
   },
   "source": [
    "This tutorial demonstrates how to classify structured data (e.g. tabular data in a CSV). We will use [Keras](https://www.tensorflow.org/guide/keras) to define the model, and [feature columns](https://www.tensorflow.org/guide/feature_columns) as a bridge to map from columns in a CSV to features used to train the model. This tutorial contains complete code to:\n",
    "\n",
    "このチュートリアルでは、（例えばCSVファイルに保存された表形式データのような）構造化されたデータをどうやって分類するかを示します。ここでは、モデルの定義に[Keras](https://www.tensorflow.org/guide/keras)を、[feature columns](https://www.tensorflow.org/guide/feature_columns)をCSVファイルの列をモデルを訓練するための特徴量にマッピングするためのブリッジとして使用します。このチュートリアルには、下記のことを行うコードすべてが含まれています。\n",
    "\n",
    "* Load a CSV file using [Pandas](https://pandas.pydata.org/).\n",
    "* Build an input pipeline to batch and shuffle the rows using [tf.data](https://www.tensorflow.org/guide/datasets).\n",
    "* Map from columns in the CSV to features used to train the model using feature columns.\n",
    "* Build, train, and evaluate a model using Keras. \n",
    "\n",
    "* [Pandas](https://pandas.pydata.org/)を使用したCSVファイルの読み込み\n",
    "* [tf.data](https://www.tensorflow.org/guide/datasets)を使用して行データをシャッフルし、バッチ化するための入力パイプライン構築\n",
    "* feature columnsを使ってCSVの列をモデル訓練用の特徴量にマップすること\n",
    "* Kerasを使ってモデルを構築し、訓練及び評価を行うこと\n",
    "\n",
    "## The Dataset\n",
    "## データセット\n",
    "\n",
    "We will use a small [dataset](https://archive.ics.uci.edu/ml/datasets/heart+Disease) provided by the Cleveland Clinic Foundation for Heart Disease. There are several hundred rows in the CSV. Each row describe a patient, and each column describes an attribute. We will use this information to predict whether a patient has heart disease, which in this dataset is a binary classification task. \n",
    "\n",
    "ここでは、Cleveland Clinic Foundation for Heart Diseaseが提供している小さな[データセット](https://archive.ics.uci.edu/ml/datasets/heart+Disease)を使用します。このCSVファイルは数百行あります。行が患者を、列がその属性を表します。この情報を使用して、患者が心臓疾患を持っているかを予測します。このデータセットの場合には二値分類タスクとなります。\n",
    "\n",
    "Following is a [description](https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/heart-disease.names) of this dataset. Notice there are both numeric and categorical columns.\n",
    "\n",
    "下記はこのデータセットの[說明](https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/heart-disease.names)です。数値列とカテゴリー列があることに注目してください。\n",
    "\n",
    ">Column| Description| Feature Type | Data Type\n",
    ">------------|--------------------|----------------------|-----------------\n",
    ">Age | Age in years | Numerical | integer\n",
    ">Sex | (1 = male; 0 = female) | Categorical | integer\n",
    ">CP | Chest pain type (0, 1, 2, 3, 4) | Categorical | integer\n",
    ">Trestbpd | Resting blood pressure (in mm Hg on admission to the hospital) | Numerical | integer\n",
    ">Chol | Serum cholestoral in mg/dl | Numerical | integer\n",
    ">FBS | (fasting blood sugar > 120 mg/dl) (1 = true; 0 = false) | Categorical | integer\n",
    ">RestECG | Resting electrocardiographic results (0, 1, 2) | Categorical | integer\n",
    ">Thalach | Maximum heart rate achieved | Numerical | integer\n",
    ">Exang | Exercise induced angina (1 = yes; 0 = no) | Categorical | integer\n",
    ">Oldpeak | ST depression induced by exercise relative to rest | Numerical | integer\n",
    ">Slope | The slope of the peak exercise ST segment | Numerical | float\n",
    ">CA | Number of major vessels (0-3) colored by flourosopy | Numerical | integer\n",
    ">Thal | 3 = normal; 6 = fixed defect; 7 = reversable defect | Categorical | string\n",
    ">Target | Diagnosis of heart disease (1 = true; 0 = false) | Classification | integer\n",
    "\n",
    ">列| 說明| 特徴量の型 | データ型\n",
    ">------------|--------------------|----------------------|-----------------\n",
    ">Age | 年齢 | 数値型 | 整数\n",
    ">Sex | (1 = 男性; 0 = 女性) | カテゴリー型 | 整数\n",
    ">CP | 胸痛のタイプ (0, 1, 2, 3, 4) | カテゴリー型 | 整数\n",
    ">Trestbpd | 安静時血圧 (単位：mm Hg 入院時) | 数値型 | 整数\n",
    ">Chol | 血清コレステロール 単位：mg/dl | 数値型 | 整数\n",
    ">FBS | (空腹時血糖 > 120 mg/dl) (1 = 真; 0 = 偽) | カテゴリー型 | 整数\n",
    ">RestECG | 安静時心電図の診断結果 (0, 1, 2) | カテゴリー型 | 整数\n",
    ">Thalach | 最大心拍数 | 数値型 | 整数\n",
    ">Exang | 運動誘発狭心症 (1 = はい; 0 = いいえ) | カテゴリー型 | 整数\n",
    ">Oldpeak | 安静時と比較した運動時のST低下 | 数値型 | 整数\n",
    ">Slope | ピーク運動STセグメントの勾配 | 数値型 | 浮動小数点数\n",
    ">CA | 蛍光透視法によって着色された主要血管の数（0−3） | 数値型 | 整数\n",
    ">Thal | 3 = 正常; 6 = 固定欠陥; 7 = 可逆的欠陥 | カテゴリー型 | 文字列\n",
    ">Target | 心臓疾患の診断 (1 = 真; 0 = 偽) | 分類 | 整数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VxyBFc_kKazA"
   },
   "source": [
    "## Import TensorFlow and other libraries\n",
    "## TensorFlow他ライブラリのインポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LuOWVJBz8a6G"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sklearn\n",
      "  Downloading https://files.pythonhosted.org/packages/1e/7a/dbb3be0ce9bd5c8b7e3d87328e79063f8b263b2b1bfa4774cb1147bfcd3f/sklearn-0.0.tar.gz\n",
      "Collecting scikit-learn (from sklearn)\n",
      "  Using cached https://files.pythonhosted.org/packages/7e/90/bfe484adb16cdad078967b2b480517ec6c1180137e26fb4b015bcb090226/scikit_learn-0.20.3-cp36-cp36m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl\n",
      "Requirement already satisfied: scipy>=0.13.3 in /Users/masatoshi/pyenvs/tf2/lib/python3.6/site-packages (from scikit-learn->sklearn) (1.2.1)\n",
      "Requirement already satisfied: numpy>=1.8.2 in /Users/masatoshi/pyenvs/tf2/lib/python3.6/site-packages (from scikit-learn->sklearn) (1.16.2)\n",
      "Building wheels for collected packages: sklearn\n",
      "  Building wheel for sklearn (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/masatoshi/Library/Caches/pip/wheels/76/03/bb/589d421d27431bcd2c6da284d5f2286c8e3b2ea3cf1594c074\n",
      "Successfully built sklearn\n",
      "Installing collected packages: scikit-learn, sklearn\n",
      "Successfully installed scikit-learn-0.20.3 sklearn-0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9dEreb4QKizj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow==2.0.0-alpha0 in /Users/masatoshi/pyenvs/tf2/lib/python3.6/site-packages (2.0.0a0)\n",
      "Requirement already satisfied: gast>=0.2.0 in /Users/masatoshi/pyenvs/tf2/lib/python3.6/site-packages (from tensorflow==2.0.0-alpha0) (0.2.2)\n",
      "Requirement already satisfied: numpy<2.0,>=1.14.5 in /Users/masatoshi/pyenvs/tf2/lib/python3.6/site-packages (from tensorflow==2.0.0-alpha0) (1.16.2)\n",
      "Requirement already satisfied: google-pasta>=0.1.2 in /Users/masatoshi/pyenvs/tf2/lib/python3.6/site-packages (from tensorflow==2.0.0-alpha0) (0.1.4)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /Users/masatoshi/pyenvs/tf2/lib/python3.6/site-packages (from tensorflow==2.0.0-alpha0) (0.7.0)\n",
      "Requirement already satisfied: astor>=0.6.0 in /Users/masatoshi/pyenvs/tf2/lib/python3.6/site-packages (from tensorflow==2.0.0-alpha0) (0.7.1)\n",
      "Requirement already satisfied: six>=1.10.0 in /Users/masatoshi/pyenvs/tf2/lib/python3.6/site-packages (from tensorflow==2.0.0-alpha0) (1.12.0)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /Users/masatoshi/pyenvs/tf2/lib/python3.6/site-packages (from tensorflow==2.0.0-alpha0) (1.0.7)\n",
      "Requirement already satisfied: tb-nightly<1.14.0a20190302,>=1.14.0a20190301 in /Users/masatoshi/pyenvs/tf2/lib/python3.6/site-packages (from tensorflow==2.0.0-alpha0) (1.14.0a20190301)\n",
      "Requirement already satisfied: tf-estimator-nightly<1.14.0.dev2019030116,>=1.14.0.dev2019030115 in /Users/masatoshi/pyenvs/tf2/lib/python3.6/site-packages (from tensorflow==2.0.0-alpha0) (1.14.0.dev2019030115)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /Users/masatoshi/pyenvs/tf2/lib/python3.6/site-packages (from tensorflow==2.0.0-alpha0) (1.19.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Users/masatoshi/pyenvs/tf2/lib/python3.6/site-packages (from tensorflow==2.0.0-alpha0) (1.1.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /Users/masatoshi/pyenvs/tf2/lib/python3.6/site-packages (from tensorflow==2.0.0-alpha0) (0.33.1)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in /Users/masatoshi/pyenvs/tf2/lib/python3.6/site-packages (from tensorflow==2.0.0-alpha0) (3.7.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /Users/masatoshi/pyenvs/tf2/lib/python3.6/site-packages (from tensorflow==2.0.0-alpha0) (1.0.9)\n",
      "Requirement already satisfied: h5py in /Users/masatoshi/pyenvs/tf2/lib/python3.6/site-packages (from keras-applications>=1.0.6->tensorflow==2.0.0-alpha0) (2.9.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /Users/masatoshi/pyenvs/tf2/lib/python3.6/site-packages (from tb-nightly<1.14.0a20190302,>=1.14.0a20190301->tensorflow==2.0.0-alpha0) (0.14.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/masatoshi/pyenvs/tf2/lib/python3.6/site-packages (from tb-nightly<1.14.0a20190302,>=1.14.0a20190301->tensorflow==2.0.0-alpha0) (3.0.1)\n",
      "Requirement already satisfied: setuptools in /Users/masatoshi/pyenvs/tf2/lib/python3.6/site-packages (from protobuf>=3.6.1->tensorflow==2.0.0-alpha0) (39.0.1)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "!pip install tensorflow==2.0.0-alpha0\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import feature_column\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KCEhSZcULZ9n"
   },
   "source": [
    "## Use Pandas to create a dataframe\n",
    "## Pandasを使ったデータフレーム作成\n",
    "\n",
    "[Pandas](https://pandas.pydata.org/) is a Python library with many helpful utilities for loading and working with structured data. We will use Pandas to download the dataset from a URL, and load it into a dataframe.\n",
    "\n",
    "[Pandas](https://pandas.pydata.org/)は、構造化データの読み込みや操作のための便利なユーティリティを持つPythonのライブラリです。ここでは、Pandasを使ってURLからデータをダウンロードし、データフレームに読み込みます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "REZ57BXCLdfG"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>145</td>\n",
       "      <td>233</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>150</td>\n",
       "      <td>0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>fixed</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>67</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>160</td>\n",
       "      <td>286</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>108</td>\n",
       "      <td>1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>normal</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>67</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>120</td>\n",
       "      <td>229</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>129</td>\n",
       "      <td>1</td>\n",
       "      <td>2.6</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>reversible</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>130</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>187</td>\n",
       "      <td>0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>normal</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>130</td>\n",
       "      <td>204</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>172</td>\n",
       "      <td>0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>normal</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  slope  \\\n",
       "0   63    1   1       145   233    1        2      150      0      2.3      3   \n",
       "1   67    1   4       160   286    0        2      108      1      1.5      2   \n",
       "2   67    1   4       120   229    0        2      129      1      2.6      2   \n",
       "3   37    1   3       130   250    0        0      187      0      3.5      3   \n",
       "4   41    0   2       130   204    0        2      172      0      1.4      1   \n",
       "\n",
       "   ca        thal  target  \n",
       "0   0       fixed       0  \n",
       "1   3      normal       1  \n",
       "2   2  reversible       0  \n",
       "3   0      normal       0  \n",
       "4   0      normal       0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "URL = 'https://storage.googleapis.com/applied-dl/heart.csv'\n",
    "dataframe = pd.read_csv(URL)\n",
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u0zhLtQqMPem"
   },
   "source": [
    "## Split the dataframe into train, validation, and test\n",
    "## データフレームを、訓練用、検証用、テスト用に分割\n",
    "\n",
    "The dataset we downloaded was a single CSV file. We will split this into train, validation, and test sets.\n",
    "\n",
    "ダウンロードしたデータセットは1つのCSVファイルです。これを、訓練用、検証用、テスト用のデータセットに分割します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YEOpw7LhMYsI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "193 train examples\n",
      "49 validation examples\n",
      "61 test examples\n"
     ]
    }
   ],
   "source": [
    "train, test = train_test_split(dataframe, test_size=0.2)\n",
    "train, val = train_test_split(train, test_size=0.2)\n",
    "print(len(train), 'train examples')\n",
    "print(len(val), 'validation examples')\n",
    "print(len(test), 'test examples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "84ef46LXMfvu"
   },
   "source": [
    "## Create an input pipeline using tf.data\n",
    "## tf.dataを使った入力パイプラインの構築\n",
    "\n",
    "Next, we will wrap the dataframes with [tf.data](https://www.tensorflow.org/guide/datasets). This will enable us  to use feature columns as a bridge to map from the columns in the Pandas dataframe to features used to train the model. If we were working with a very large CSV file (so large that it does not fit into memory), we would use tf.data to read it from disk directly. That is not covered in this tutorial.\n",
    "\n",
    "次に、[tf.data](https://www.tensorflow.org/guide/datasets)を使ってデータフレームをラップします。こうすることで、feature columns をPandasデータフレームの列をモデル訓練用の特徴量へのマッピングに使うことができます。（メモリに収まらないぐらいの）非常に大きなCSVファイルを扱う場合には、tf.dataを使ってディスクから直接CSVファイルを読み込むことになります。この方法は、このチュートリアルでは扱いません。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NkcaMYP-MsRe"
   },
   "outputs": [],
   "source": [
    "# A utility method to create a tf.data dataset from a Pandas Dataframe\n",
    "# Pandasデータフレームからtf.dataデータセットを作るためのユーティリティメソッド\n",
    "def df_to_dataset(dataframe, shuffle=True, batch_size=32):\n",
    "  dataframe = dataframe.copy()\n",
    "  labels = dataframe.pop('target')\n",
    "  ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n",
    "  if shuffle:\n",
    "    ds = ds.shuffle(buffer_size=len(dataframe))\n",
    "  ds = ds.batch(batch_size)\n",
    "  return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CXbbXkJvMy34"
   },
   "outputs": [],
   "source": [
    "batch_size = 5 # A small batch sized is used for demonstration purposes\n",
    "train_ds = df_to_dataset(train, batch_size=batch_size)\n",
    "val_ds = df_to_dataset(val, shuffle=False, batch_size=batch_size)\n",
    "test_ds = df_to_dataset(test, shuffle=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qRLGSMDzM-dl"
   },
   "source": [
    "## Understand the input pipeline\n",
    "## 入力パイプラインを理解する\n",
    "\n",
    "Now that we have created the input pipeline, let's call it to see the format of the data it returns. We have used a small batch size to keep the output readable.\n",
    "\n",
    "入力パイプラインを構築したので、それが返すデータのフォーマットを見るために呼び出してみましょう。出力を読みやすくするためにバッチサイズを小さくしてあります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CSBo3dUVNFc9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every feature: ['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach', 'exang', 'oldpeak', 'slope', 'ca', 'thal']\n",
      "A batch of ages: tf.Tensor([50 54 48 39 54], shape=(5,), dtype=int32)\n",
      "A batch of targets: tf.Tensor([0 0 0 0 0], shape=(5,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for feature_batch, label_batch in train_ds.take(1):\n",
    "  print('Every feature:', list(feature_batch.keys()))\n",
    "  print('A batch of ages:', feature_batch['age'])\n",
    "  print('A batch of targets:', label_batch )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OT5N6Se-NQsC"
   },
   "source": [
    "We can see that the dataset returns a dictionary of column names (from the dataframe) that map to column values from rows in the dataframe.\n",
    "\n",
    "データセットが（データフレーム由来の）列名からなるディクショナリを返すことがわかります。列名から、データフレームの行に含まれる列の値が得られます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ttIvgLRaNoOQ"
   },
   "source": [
    "## Demonstrate several types of feature column\n",
    "## feature columnの様々な型の例\n",
    "\n",
    "TensorFlow provides many types of feature columns. In this section, we will create several types of feature columns, and demonstrate how they transform a column from the dataframe.\n",
    "\n",
    "TensorFlowにはたくさんの型のfeature columnがあります。このセクションでは、いくつかの型のfeature columnを作り、データフレームの列をどのように変換しているかを示します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mxwiHFHuNhmf"
   },
   "outputs": [],
   "source": [
    "# We will use this batch to demonstrate several types of feature columns\n",
    "# いくつかの型のfeature columnを例示するためこのバッチを使用する\n",
    "example_batch = next(iter(train_ds))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0wfLB8Q3N3UH"
   },
   "outputs": [],
   "source": [
    "# A utility method to create a feature column\n",
    "# and to transform a batch of data\n",
    "# feature columnを作りデータのバッチを変換する\n",
    "# ユーティリティメソッド\n",
    "def demo(feature_column):\n",
    "  feature_layer = layers.DenseFeatures(feature_column)\n",
    "  print(feature_layer(example_batch).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q7OEKe82N-Qb"
   },
   "source": [
    "### Numeric columns\n",
    "### 数値コラム\n",
    "\n",
    "The output of a feature column becomes the input to the model (using the demo function defined above, we will be able to see exactly how each column from the dataframe is transformed). A [numeric column](https://www.tensorflow.org/api_docs/python/tf/feature_column/numeric_column) is the simplest type of column. It is used to represent real valued features. When using this column, your model will receive the column value from the dataframe unchanged. \n",
    "\n",
    "feature columnの出力はモデルへの入力になります（上記で定義したdemo関数を使うと、データフレームの列がどのように変換されるかをつぶさに見ることができます）。[数値コラム](https://www.tensorflow.org/api_docs/python/tf/feature_column/numeric_column)は、最も単純な型のコラムです。数値コラムは実数特徴量を表現するのに使われます。このコラムを使う場合、モデルにはデータフレームの列の値がそのまま渡されます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QZTZ0HnHOCxC"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0402 14:08:27.281308 4651120064 deprecation.py:323] From /Users/masatoshi/pyenvs/tf2/lib/python3.6/site-packages/tensorflow/python/feature_column/feature_column_v2.py:2758: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[50.]\n",
      " [54.]\n",
      " [48.]\n",
      " [39.]\n",
      " [54.]]\n"
     ]
    }
   ],
   "source": [
    "age = feature_column.numeric_column(\"age\")\n",
    "demo(age)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7a6ddSyzOKpq"
   },
   "source": [
    "In the heart disease dataset, most columns from the dataframe are numeric.\n",
    "\n",
    "心臓疾患データセットでは、データフレームのほとんどの列が数値型です。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IcSxUoYgOlA1"
   },
   "source": [
    "### Bucketized columns\n",
    "### バケット型コラム\n",
    "\n",
    "Often, you don't want to feed a number directly into the model, but instead split its value into different categories based on numerical ranges. Consider raw data that represents a person's age. Instead of representing age as a numeric column, we could split the age into several buckets using a [bucketized column](https://www.tensorflow.org/api_docs/python/tf/feature_column/bucketized_column). Notice the one-hot values below describe which age range each row matches.\n",
    "\n",
    "数値をそのままモデルに入力するのではなく、値の範囲に基づいた異なるカテゴリーに分割したいことがあります。例えば、人の年齢を表す生データを考えてみましょう。[バケット型コラム](https://www.tensorflow.org/api_docs/python/tf/feature_column/bucketized_column)を使うと年齢を数値コラムとして表現するのではなく、年齢をいくつかのバケツに分割できます。下記のワンホット値が、各行がどの年齢範囲にあるかを表していることに注目してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wJ4Wt3SAOpTQ"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0402 14:09:25.861896 4651120064 deprecation.py:323] From /Users/masatoshi/pyenvs/tf2/lib/python3.6/site-packages/tensorflow/python/feature_column/feature_column_v2.py:2902: to_int64 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "age_buckets = feature_column.bucketized_column(age, boundaries=[18, 25, 30, 35, 40, 45, 50, 55, 60, 65])\n",
    "demo(age_buckets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r1tArzewPb-b"
   },
   "source": [
    "### Categorical columns\n",
    "### カテゴリー型コラム\n",
    "\n",
    "In this dataset, thal is represented as a string (e.g. 'fixed', 'normal', or 'reversible'). We cannot feed strings directly to a model. Instead, we must first map them to numeric values. The categorical vocabulary columns provide a way to represent strings as a one-hot vector (much like you have seen above with age buckets). The vocabulary can be passed as a list using [categorical_column_with_vocabulary_list](https://www.tensorflow.org/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_list), or loaded from a file using [categorical_column_with_vocabulary_file](https://www.tensorflow.org/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_file). \n",
    "\n",
    "このデータセットでは、Thalは（'fixed'、'normal'、'reversible'のような）文字列として表現されています。文字列を直接モデルに入力することはできません。まず、文字列を数値にマッピングする必要があります。categorical vocabulary コラムを使うと、（上記で示した年齢バケットのように）文字列をワンホットベクトルとして表現することができます。カテゴリーを表す語彙(vocabulary)は[categorical_column_with_vocabulary_list](https://www.tensorflow.org/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_list)を使ってリストで渡すか、[categorical_column_with_vocabulary_file](https://www.tensorflow.org/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_file)を使ってファイルから読み込むことができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DJ6QnSHkPtOC"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0402 14:10:34.299263 4651120064 deprecation.py:323] From /Users/masatoshi/pyenvs/tf2/lib/python3.6/site-packages/tensorflow/python/feature_column/feature_column_v2.py:4307: IndicatorColumn._variable_shape (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "W0402 14:10:34.300477 4651120064 deprecation.py:323] From /Users/masatoshi/pyenvs/tf2/lib/python3.6/site-packages/tensorflow/python/feature_column/feature_column_v2.py:4362: VocabularyListCategoricalColumn._num_buckets (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "thal = feature_column.categorical_column_with_vocabulary_list(\n",
    "      'thal', ['fixed', 'normal', 'reversible'])\n",
    "\n",
    "thal_one_hot = feature_column.indicator_column(thal)\n",
    "demo(thal_one_hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dxQloQ9jOoXL"
   },
   "source": [
    "In a more complex dataset, many columns would be categorical (e.g. strings). Feature columns are most valuable when working with categorical data. Although there is only one categorical column in this dataset, we will use it to demonstrate several important types of feature columns that you could use when working with other datasets.\n",
    "\n",
    "より複雑なデータセットでは、たくさんの列がカテゴリー型（例えば文字列）であることでしょう。feature column はカテゴリー型データを扱う際に最も役に立ちます。このデータセットでは、カテゴリー型コラムは1つだけですが、他のデータセットを扱う際に使用できるいくつかの重要な型のfeature columnを紹介するために、この列を使用することにします。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LEFPjUr6QmwS"
   },
   "source": [
    "### Embedding columns\n",
    "### 埋め込み型コラム\n",
    "\n",
    "Suppose instead of having just a few possible strings, we have thousands (or more) values per category. For a number of reasons, as the number of categories grow large, it becomes infeasible to train a neural network using one-hot encodings. We can use an embedding column to overcome this limitation. Instead of representing the data as a one-hot vector of many dimensions, an [embedding column](https://www.tensorflow.org/api_docs/python/tf/feature_column/embedding_column) represents that data as a lower-dimensional, dense vector in which each cell can contain any number, not just 0 or 1. The size of the embedding (8, in the example below) is a parameter that must be tuned.\n",
    "\n",
    "数種類の候補となる文字列ではなく、カテゴリー毎に数千（あるいはそれ以上）の値があるとしましょう。カテゴリーの数が多くなってくると、様々な理由から、ワンホットエンコーディングを使ってニューラルネットワークを訓練することが難しくなります。埋込み型コラムを使うと、こうした制約を克服することが可能です。[埋込み型コラム](https://www.tensorflow.org/api_docs/python/tf/feature_column/embedding_column)は、データを多次元のワンホットベクトルとして表すのではなく、セルの値が0か1かだけではなく、どんな数値でもとれるような密な低次元ベクトルとして表現します。埋め込みのサイズ（下記の例では8）は、チューニングが必要なパラメータです。\n",
    "\n",
    "Key point: using an embedding column is best when a categorical column has many possible values. We are using one here for demonstration purposes, so you have a complete example you can modify for a different dataset in the future.\n",
    "\n",
    "キーポイント：カテゴリー型コラムがたくさんの選択肢を持つ場合、埋め込み型コラムを使用することが最善の方法です。ここでは例を一つ示しますので、今後様々なデータセットを扱う際には、この例を参考にしてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hSlohmr2Q_UU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.31042248 -0.12490357  0.26811242 -0.17417313 -0.33425182  0.55663043\n",
      "  -0.6095312   0.10499681]\n",
      " [ 0.1083658  -0.06921068  0.03248625 -0.08198062 -0.5868681  -0.46379614\n",
      "   0.36950853 -0.2830798 ]\n",
      " [ 0.1083658  -0.06921068  0.03248625 -0.08198062 -0.5868681  -0.46379614\n",
      "   0.36950853 -0.2830798 ]\n",
      " [ 0.31042248 -0.12490357  0.26811242 -0.17417313 -0.33425182  0.55663043\n",
      "  -0.6095312   0.10499681]\n",
      " [ 0.31042248 -0.12490357  0.26811242 -0.17417313 -0.33425182  0.55663043\n",
      "  -0.6095312   0.10499681]]\n"
     ]
    }
   ],
   "source": [
    "# Notice the input to the embedding column is the categorical column\n",
    "# we previously created\n",
    "thal_embedding = feature_column.embedding_column(thal, dimension=8)\n",
    "demo(thal_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "urFCAvTVRMpB"
   },
   "source": [
    "### Hashed feature columns\n",
    "### ハッシュ化特徴コラム\n",
    "\n",
    "Another way to represent a categorical column with a large number of values is to use a [categorical_column_with_hash_bucket](https://www.tensorflow.org/api_docs/python/tf/feature_column/categorical_column_with_hash_bucket). This feature column calculates a hash value of the input, then selects one of the `hash_bucket_size` buckets to encode a string. When using this column, you do not need to provide the vocabulary, and you can choose to make the number of hash_buckets significantly smaller than the number of actual categories to save space.\n",
    "\n",
    "値の種類が多いカテゴリー型コラムを表現するもう一つの方法が、[categorical_column_with_hash_bucket](https://www.tensorflow.org/api_docs/python/tf/feature_column/categorical_column_with_hash_bucket)を使う方法です。このfeature columnは文字列をエンコードするために入力のハッシュ値を計算し、`hash_bucket_size`個のバケットの中から1つを選択します。このコラムを使用する場合には、語彙を用意する必要はありません。また、スペースの節約のために、実際のカテゴリー数に比べて極めて少ないバケット数を選択することも可能です。\n",
    "\n",
    "Key point: An important downside of this technique is that there may be collisions in which different strings are mapped to the same bucket. In practice, this can work well for some datasets regardless.\n",
    "\n",
    "キーポイント：この手法の重要な欠点の一つは、異なる文字列が同じバケットにマッピンされるハッシュ値の衝突が起きることです。実務上は、データセットによっては、この問題を無視できることがあります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YHU_Aj2nRRDC"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0402 14:14:19.700885 4651120064 deprecation.py:323] From /Users/masatoshi/pyenvs/tf2/lib/python3.6/site-packages/tensorflow/python/feature_column/feature_column_v2.py:4362: HashedCategoricalColumn._num_buckets (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "thal_hashed = feature_column.categorical_column_with_hash_bucket(\n",
    "      'thal', hash_bucket_size=1000)\n",
    "demo(feature_column.indicator_column(thal_hashed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fB94M27DRXtZ"
   },
   "source": [
    "### Crossed feature columns\n",
    "### クロスフィーチャーコラム\n",
    "\n",
    "Combining features into a single feature, better known as [feature crosses](https://developers.google.com/machine-learning/glossary/#feature_cross), enables a model to learn separate weights for each combination of features. Here, we will create a new feature that is the cross of age and thal. Note that `crossed_column` does not build the full table of all possible combinations (which could be very large). Instead, it is backed by a `hashed_column`, so you can choose how large the table is.\n",
    "\n",
    "複数の特徴量をまとめて1つの特徴量にする、[フィーチャークロス](https://developers.google.com/machine-learning/glossary/#feature_cross)として知られている手法は、モデルが特徴量の組み合わせの一つ一つに別々の重みを学習することを可能にします。ここでは年齢とThalをクロスさせて新しい特徴量を作ってみます。交差列(`crossed_column`)が、起こりうるすべての組み合わせ全体のテーブル（これは非常に大きくなる可能性があります）を作るものではないことに注意してください。クロスフィーチャーコラムは、代わりにバックエンドとしてハッシュ化コラムを使用しているため、テーブルの大きさを選択することができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oaPVERd9Rep6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0402 14:14:26.447788 4651120064 deprecation.py:323] From /Users/masatoshi/pyenvs/tf2/lib/python3.6/site-packages/tensorflow/python/feature_column/feature_column_v2.py:4362: CrossedColumn._num_buckets (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "crossed_feature = feature_column.crossed_column([age_buckets, thal], hash_bucket_size=1000)\n",
    "demo(feature_column.indicator_column(crossed_feature))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ypkI9zx6Rj1q"
   },
   "source": [
    "## Choose which columns to use\n",
    "## 使用するコラムを選択する\n",
    "\n",
    "We have seen how to use several types of feature coilumns. Now we will use them to train a model. The goal of this tutorial is to show you the complete code (e.g. mechanics) needed to work with feature columns. We have selected a few columns to train our model below arbitrarily.\n",
    "\n",
    "これまで、いくつかのfeature columnの使い方を見てきました。いよいよモデルの訓練にそれらを使用することにします。このチュートリアルの目的は、feature columnを使うのに必要な完全なコード（いわば力学）を示すことです。以下ではモデルを訓練するための列を適当に選びました。\n",
    "\n",
    "Key point: If your aim is to build an accurate model, try a larger dataset of your own, and think carefully about which features are the most meaningful to include, and how they should be represented.\n",
    "\n",
    "キーポイント：正確なモデルを構築するのが目的である場合には、できるだけ大きなデータセットを使用して、どの特徴量を含めるのがもっとも意味があるのかや、それらをどう表現したらよいかを、慎重に検討してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4PlLY7fORuzA"
   },
   "outputs": [],
   "source": [
    "feature_columns = []\n",
    "\n",
    "# numeric cols\n",
    "# 数値コラム\n",
    "for header in ['age', 'trestbps', 'chol', 'thalach', 'oldpeak', 'slope', 'ca']:\n",
    "  feature_columns.append(feature_column.numeric_column(header))\n",
    "\n",
    "# bucketized cols\n",
    "# バケット化コラム\n",
    "age_buckets = feature_column.bucketized_column(age, boundaries=[18, 25, 30, 35, 40, 45, 50, 55, 60, 65])\n",
    "feature_columns.append(age_buckets)\n",
    "\n",
    "# indicator cols\n",
    "# インジケーター（カテゴリー型）コラム\n",
    "thal = feature_column.categorical_column_with_vocabulary_list(\n",
    "      'thal', ['fixed', 'normal', 'reversible'])\n",
    "thal_one_hot = feature_column.indicator_column(thal)\n",
    "feature_columns.append(thal_one_hot)\n",
    "\n",
    "# embedding cols\n",
    "# 埋め込み型コラム\n",
    "thal_embedding = feature_column.embedding_column(thal, dimension=8)\n",
    "feature_columns.append(thal_embedding)\n",
    "\n",
    "# crossed cols\n",
    "# クロスフィーチャーコラム\n",
    "crossed_feature = feature_column.crossed_column([age_buckets, thal], hash_bucket_size=1000)\n",
    "crossed_feature = feature_column.indicator_column(crossed_feature)\n",
    "feature_columns.append(crossed_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M-nDp8krS_ts"
   },
   "source": [
    "### Create a feature layer\n",
    "### 特徴量層の構築\n",
    "\n",
    "Now that we have defined our feature columns, we will use a [DenseFeatures](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/layers/DenseFeatures) layer to input them to our Keras model.\n",
    "\n",
    "feature columnを定義し終わったので、次に[DenseFeatures](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/layers/DenseFeatures)層を使ってKerasモデルへの入力とします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6o-El1R2TGQP"
   },
   "outputs": [],
   "source": [
    "feature_layer = tf.keras.layers.DenseFeatures(feature_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8cf6vKfgTH0U"
   },
   "source": [
    "Earlier, we used a small batch size to demonstrate how feature columns worked. We create a new input pipeline with a larger batch size.\n",
    "\n",
    "これまでは、feature columnの働きを見るため、小さなバッチサイズを使ってきました。ここではもう少し大きなバッチサイズの新しい入力パイプラインを作ります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gcemszoGSse_"
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_ds = df_to_dataset(train, batch_size=batch_size)\n",
    "val_ds = df_to_dataset(val, shuffle=False, batch_size=batch_size)\n",
    "test_ds = df_to_dataset(test, shuffle=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bBx4Xu0eTXWq"
   },
   "source": [
    "## Create, compile, and train the model\n",
    "## モデルの構築、コンパイルと訓練"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_YJPPb3xTPeZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "7/7 [==============================] - 1s 123ms/step - loss: 2.3117 - accuracy: 0.5700 - val_loss: 0.6291 - val_accuracy: 0.6735\n",
      "Epoch 2/5\n",
      "7/7 [==============================] - 0s 39ms/step - loss: 0.9399 - accuracy: 0.5579 - val_loss: 1.1722 - val_accuracy: 0.7143\n",
      "Epoch 3/5\n",
      "7/7 [==============================] - 0s 40ms/step - loss: 0.5785 - accuracy: 0.7556 - val_loss: 0.6076 - val_accuracy: 0.7347\n",
      "Epoch 4/5\n",
      "7/7 [==============================] - 0s 55ms/step - loss: 0.5179 - accuracy: 0.7310 - val_loss: 0.7435 - val_accuracy: 0.6327\n",
      "Epoch 5/5\n",
      "7/7 [==============================] - 0s 42ms/step - loss: 0.6059 - accuracy: 0.6948 - val_loss: 0.6616 - val_accuracy: 0.7347\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1346d6668>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "  feature_layer,\n",
    "  layers.Dense(128, activation='relu'),\n",
    "  layers.Dense(128, activation='relu'),\n",
    "  layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_ds, \n",
    "          validation_data=val_ds, \n",
    "          epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GnFmMOW0Tcaa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 33ms/step - loss: 0.3726 - accuracy: 0.8197\n",
      "Accuracy 0.8196721\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(test_ds)\n",
    "print(\"Accuracy\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3bdfbq20V6zu"
   },
   "source": [
    "Key point: You will typically see best results with deep learning with much larger and more complex datasets. When working with a small dataset like this one, we recommend using a decision tree or random forest as a strong baseline. The goal of this tutorial is not to train an accurate model, but to demonstrate the mechanics of working with structured data, so you have code to use as a starting point when working with your own datasets in the future.\n",
    "\n",
    "キーポイント：一般的に、ディープラーニングが最良の結果となるのは、もっと大きくて、もっと複雑なデータセットです。この例のように小さなデータセットを使用する際には、しっかりしたベースラインとして、決定木やランダムフォレストを使うことをおすすめします。このチュートリアルの目的は、訓練により正確なモデルを得ることではなく、構造化データの使い方をデモすることです。今後ご自分のデータセットに取り組まれる際の出発点として、これらのコードをお使いください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SotnhVWuHQCw"
   },
   "source": [
    "## Next steps\n",
    "## 次のステップ\n",
    "\n",
    "The best way to learn more about classifying structured data is to try it yourself. We suggest finding another dataset to work with, and training a model to classify it using code similar to the above. To improve accuracy, think carefully about which features to include in your model, and how they should be represented.\n",
    "\n",
    "構造化データの分類について更に多くのことを学ぶためには、自分自身で試してみることです。別のデータセットを見つけて、上記と同様のコードを使って、それを分類するモデルを訓練してみてください。正解率を上げるためには、モデルにどの特徴量を含めたらよいかや、その特徴量をどのように表現すべきかをじっくり考えてください。"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "feature_columns.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
